{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import cooc\n",
    "import pickle_vocab\n",
    "import glove_template\n",
    "import glove_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/romy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# file paths (NOT FULL DATASETS FOR NOW!!!)\n",
    "DATASETS_FOLDER = 'twitter-datasets'\n",
    "POS_FILE = os.path.join(DATASETS_FOLDER, 'train_pos.txt')\n",
    "NEG_FILE = os.path.join(DATASETS_FOLDER, 'train_neg.txt')\n",
    "TEST_FILE = os.path.join(DATASETS_FOLDER, 'test_data.txt')\n",
    "VOCAB_FILE = 'vocab.pkl'\n",
    "EMBEDDINGS_FILE = 'embeddings.npy'\n",
    "\n",
    "# download nltk ressources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load vocabulary\n",
    "with open(VOCAB_FILE, 'rb') as f:\n",
    "    vocab = pickle.load(f)  # word -> index\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = np.load(EMBEDDINGS_FILE)  # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        tweets = f.readlines()\n",
    "    # Remove newline characters\n",
    "    tweets = [tweet.strip() for tweet in tweets]\n",
    "    return tweets\n",
    "\n",
    "def load_test_tweets(file_path):\n",
    "    tweet_ids = []\n",
    "    tweets = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Each line is in the format: \"<tweet_id>,<tweet_text>\"\n",
    "            # In this dataset, test tweets are numbered but may not include commas\n",
    "            tweet = line.strip()\n",
    "            if tweet:\n",
    "                tweet_ids.append(len(tweet_ids) + 1)  # Assuming tweet IDs are 1-based indices\n",
    "                tweets.append(tweet)\n",
    "    return tweet_ids, tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data (NOT FULL DATASETS FOR NOW!!!)\n",
    "pos_tweets = load_tweets(POS_FILE)\n",
    "neg_tweets = load_tweets(NEG_FILE)\n",
    "test_ids, test_tweets = load_test_tweets(TEST_FILE)\n",
    "\n",
    "# labels\n",
    "pos_labels = [1] * len(pos_tweets)\n",
    "neg_labels = [0] * len(neg_tweets)\n",
    "\n",
    "# concatenation\n",
    "all_tweets = pos_tweets + neg_tweets\n",
    "all_labels = pos_labels + neg_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent each tweet as an average of its word embeddings\n",
    "def tweet_to_embedding(tweet, vocab, embeddings):\n",
    "    words = tweet.split()  # Tweets are already tokenized\n",
    "    indices = [vocab.get(word) for word in words if word in vocab]\n",
    "    if not indices:\n",
    "        # If no words in vocab, return zero vector\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    word_vectors = embeddings[indices]\n",
    "    tweet_embedding = np.mean(word_vectors, axis=0)\n",
    "    return tweet_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for all tweets\n",
    "tweet_embeddings = np.array([tweet_to_embedding(tweet, vocab, embeddings) for tweet in all_tweets])\n",
    "test_embeddings = np.array([tweet_to_embedding(tweet, vocab, embeddings) for tweet in test_tweets])\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(tweet_embeddings, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (180000, 20)\n",
      "Number of samples: 180000\n",
      "Number of features: 20\n"
     ]
    }
   ],
   "source": [
    "# For training data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Number of samples:\", X_train.shape[0])\n",
    "print(\"Number of features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training set: Counter({1: 90039, 0: 89961})\n",
      "Number of positive samples: 90039 (50.02%)\n",
      "Number of negative samples: 89961 (49.98%)\n",
      "Class distribution in training set: {0: 89961, 1: 90039}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming y_train contains your training labels\n",
    "class_counts = Counter(y_train)\n",
    "print(\"Class distribution in training set:\", class_counts)\n",
    "\n",
    "# For a binary classification problem\n",
    "num_positive = class_counts[1]\n",
    "num_negative = class_counts[0]\n",
    "total_samples = num_positive + num_negative\n",
    "\n",
    "print(f\"Number of positive samples: {num_positive} ({(num_positive/total_samples)*100:.2f}%)\")\n",
    "print(f\"Number of negative samples: {num_negative} ({(num_negative/total_samples)*100:.2f}%)\")\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(\"Class distribution in training set:\", class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No class imbalance (approx 50/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(dual=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize classifiers\n",
    "logistic_regression = LogisticRegression(max_iter=1000)\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = LinearSVC(dual=False)\n",
    "\n",
    "# Train classifiers\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if we want to test Naive Bayes as well, we must either replace MultinomialNB with GaussianNB (because multinomialNB doesn't handle negative values) or use feature representations w/non-negative counts (bag-of-words, TF-IDF vectors) with MultinomialNB (which is probably more suitable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of each classifier and select the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    y_preds = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_preds)\n",
    "    report = classification_report(y_test, y_preds)\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best-performing model: Random Forest\n",
      "Accuracy: 0.6345\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.59      0.62     10039\n",
      "           1       0.62      0.68      0.65      9961\n",
      "\n",
      "    accuracy                           0.63     20000\n",
      "   macro avg       0.64      0.63      0.63     20000\n",
      "weighted avg       0.64      0.63      0.63     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate classifiers\n",
    "logistic_accuracy, logistic_report = evaluate_classifier(logistic_regression, X_val, y_val)\n",
    "random_forest_accuracy, random_forest_report = evaluate_classifier(random_forest, X_val, y_val)\n",
    "svm_accuracy, svm_report = evaluate_classifier(svm, X_val, y_val)\n",
    "\n",
    "# Select the best-performing model\n",
    "best_model = max([(logistic_accuracy, 'Logistic Regression'), \n",
    "                  (random_forest_accuracy, 'Random Forest'), \n",
    "                  (svm_accuracy, 'SVM')], key=lambda x: x[0])\n",
    "\n",
    "print(\"Best-performing model:\", best_model[1])\n",
    "print(\"Accuracy:\", best_model[0])\n",
    "print(\"Classification report:\\n\", evaluate_classifier(globals()[best_model[1].lower().replace(' ', '_')], X_val, y_val)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for the best-performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing model among Random Forest, SVM and Logistic Regression is apparently Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "# Create the parameters grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [None, 10, 50],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation with randomized search\n",
    "\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    cv=cv_strategy,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    scoring='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romy/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 36 is smaller than n_iter=50. Running 36 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/romy/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/romy/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/romy/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/romy/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=100; total time= 4.0min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=100; total time= 4.0min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=100; total time= 4.0min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=300; total time=11.0min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=300; total time=10.4min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=300; total time=10.5min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=100; total time= 3.3min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=500; total time=17.0min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=100; total time= 3.4min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=100; total time= 3.4min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=500; total time=17.0min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=300; total time=10.2min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=sqrt, n_estimators=500; total time=17.1min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=300; total time=10.1min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=300; total time=10.1min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=100; total time= 1.9min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=100; total time= 1.9min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=100; total time= 1.9min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=500; total time=16.7min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=500; total time=16.7min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=300; total time= 5.8min\n",
      "[CV] END criterion=gini, max_depth=None, max_features=log2, n_estimators=500; total time=16.9min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=300; total time= 5.8min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=300; total time= 5.8min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=100; total time= 1.9min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=100; total time= 1.9min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=500; total time= 9.6min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=100; total time= 1.9min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=500; total time= 9.4min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=sqrt, n_estimators=500; total time= 8.8min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=300; total time= 4.7min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=300; total time= 4.6min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=300; total time= 4.5min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=100; total time= 2.6min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=500; total time= 7.4min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=100; total time= 2.6min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=500; total time= 7.4min\n",
      "[CV] END criterion=gini, max_depth=10, max_features=log2, n_estimators=500; total time= 7.4min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=100; total time= 2.7min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=300; total time= 7.8min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=300; total time= 7.8min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=300; total time= 7.9min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, n_estimators=100; total time= 2.5min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=log2, n_estimators=100; total time= 2.6min\n",
      "[CV] END criterion=gini, max_depth=50, max_features=sqrt, n_estimators=500; total time=13.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/romy/programmation/Ma1/Ma1 ML/project 2/project2.ipynb Cellule 30\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/romy/programmation/Ma1/Ma1%20ML/project%202/project2.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# CAN BE REALLY LONG!!!\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/romy/programmation/Ma1/Ma1%20ML/project%202/project2.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rf_random\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1766\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1765\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1766\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1767\u001b[0m         ParameterSampler(\n\u001b[1;32m   1768\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[1;32m   1769\u001b[0m         )\n\u001b[1;32m   1770\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m         clone(base_estimator),\n\u001b[1;32m    841\u001b[0m         X,\n\u001b[1;32m    842\u001b[0m         y,\n\u001b[1;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    849\u001b[0m     )\n\u001b[1;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    852\u001b[0m     )\n\u001b[1;32m    853\u001b[0m )\n\u001b[1;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CAN BE REALLY LONG!!!\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best parameters\n",
    "print(\"Best Parameters found:\")\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "# Evaluate the best estimator on the validation set\n",
    "best_rf = rf_random.best_estimator_\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "\n",
    "# Compute accuracy\n",
    "best_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy after tuning: {best_accuracy:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this: we can perform a more focused search with GridSearchCV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bed51e97e87222c3794562b6745ca35a5928cf6202ff130a34ca0ac1950e0064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
